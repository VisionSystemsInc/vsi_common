#!/usr/bin/env false bash

#*# tests/testlib

#**
# ===========
# VSI Testlib
# ===========
#
# .. default-domain:: bash
#
# .. file:: testlib.bsh
#
# Simple shell command language test library
#
# :Copyright: Original version: (c) 2011-13 by Ryan Tomayko <http://tomayko.com>
#
#             License: MIT
#
# ## Writing unit tests
#
# Testlib gives you a number of basic unit test functionality from bash, including:
#
# - Running tests in subshells to prevent environment variable pollution.
# - An automatically self deleting :envvar:`TRASHDIR` for all the tests
# - An automatically self deleting :envvar:`TESTDIR` for each individual test (in the :envvar:`TRASHDIR`)
# - A tally of successfully run and failed tests, in addition to expected failures, unexpected successes, required failures, and skipped tests
# - Individual est times: :envvar:`TESTLIB_SHOW_TIMING`
# - A user defined :func:`setup` function run before the first test in a file
# - A user defined :func:`teardown` function run after the last test in a file
# - Keep temporary directories for debugging: :envvar:`TESTLIB_KEEP_TEMP_DIRS`
# - Pause before deleting temporary directories if there is a failure for inspection: :envvar:`TESTLIB_KEEP_PAUSE_AFTER_ERROR`
# - Run only a single test by its description: :envvar:`TESTLIB_RUN_SINGLE_TEST`
# - Regular expression to skip tests by description: :envvar:`TESTLIB_SKIP_TESTS`
# - Controlled stderr/stdout redirection :envvar:`TESTLIB_REDIRECT_OUTPUT`
# - Stop testing after ``N`` failures :envvar:`TESTLIB_STOP_AFTER_FAILS`
# - Custom PS4 in trace using :envvar:`TESTLIB_PS4`
# - Ability to conditionally skip a test by calling :func:`skip_next_test` in any condition check
# - Track files outside the :envvar:`TRASHDIR` with :func:`ttouch` so that they will be automatically deleted during cleanup.
# - Other helper functions like :func:`test_utils.bsh not`, :func:`test_utils.bsh not_s`, :func:`test_utils.bsh assert_array_values`, :func:`test_utils.bsh assert_array_regex_values`, :func:`test_utils.bsh assert_array_contiguous`
# - Auto discover and run tests script: :file:`run_tests`
#
# .. rubric:: Example
#
# .. code-block:: bash
#   :caption: Tests must follow the basic form:
#
#   source testlib.bsh
#
#   begin_test "the thing"
#   (
#        set -e
#        echo "hello"
#        [ 1 == 1 ] # this is ok
#        # However, the following needs "|| false" because on bash 3.2 and 4.0
#        # there is a bug where [[ ]] will fail, and bash knows it fails ($?),
#        # but "set -e" does not count this as an error. This is fixed in bash 4.1
#        [[ 1 == 1 ]] || false
#        false
#   )
#   end_test
#
#   Any command that evaluates to false "fails" the test. When a test fails, its stdout, stderr, and call trace are printed out.
#
# .. rubric:: Bugs
#
# On darling: when debugging a unit test error, sometimes the printout is cut off, making it difficult to do "printf debugging." While the cause and scope of this is unknown, a work around that sometimes works is
#
# .. code-block:: bash
#
#     runtests 2>&1 | less -R
#
# ## Test status
#
# Every test will print out a line with its name, and the status of the test run.
#
# Possible results of a test:
#
# - ``OK`` - The test passed!
# - ``FAILED`` - The test did not pass. A trace is printed out for debugging.
# - ``SETUP FAILURE`` - Each test must call :func:`setup_test`, and it was not detected for this test.
# - ``SKIPPED`` - The test was not run.
# - ``FAIL REQUIRED`` - A test successfully failed as it is required to. If the test succeeds, it is treated as a failed test.
# - ``FAIL EXPECTED`` - A test successfully failed as it is expected to. If the test succeeds, it is treated as an unexpected success.
# - ``SHOULD HAVE FAILED ELSEWHERE`` - A required or expected to fail test failed in an the wrong area of the code. There is something wrong with the test, and a trace is printed out for debugging.
# - ``SHOULD HAVE FAILED`` - A required to fail test did not fail. There is something wrong with the test, and a trace is printed out for debugging.
# - ``UNEXPECTED SUCCESS`` - An expected to fail test never actually failed. This doesn't count as a failure, but a middle ground in its own category.
# - ``REQUIRED FAILURE SETUP ERROR`` - A required to fail test did not call :func:`begin_fail_zone` and is considered setup incorrectly.
# - ``EXPECTED FAILURE SETUP ERROR`` - An expected to fail test did not call :func:`begin_fail_zone` and is considered setup incorrectly.
#**

if [ -z "${VSI_COMMON_DIR+set}" ]; then
  VSI_COMMON_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.."; pwd)"
fi

set -eu

if [ "${TESTLIB_SHOW_TIMING-0}" == "1" ] || [[ ${OSTYPE-} = darwin* ]]; then
  . "${VSI_COMMON_DIR}/linux/time_tools.bsh"
fi
source "${VSI_COMMON_DIR}/linux/file_tools.bsh"
source "${VSI_COMMON_DIR}/linux/dir_tools.bsh"
source "${VSI_COMMON_DIR}/tests/test_colors.sh"
source "${VSI_COMMON_DIR}/linux/signal_tools.bsh"
source "${VSI_COMMON_DIR}/linux/set_flags.bsh"
source "${VSI_COMMON_DIR}/linux/compat.bsh"

# create a temporary work space
TRASHDIR="$(mktemp -d -t $(basename "${0}")-${$}.XXXXXXXX)"

# keep track of num tests and failures
tests=0
failures=0
expected_failures=0
unexpected_successes=0
required_failures=0
skipped=0

#**
# .. function:: setup
#
# Function run before the first test. Must be declared before the first test is called in the test file, or else it will not be discovered in time.
#
# .. note::
#   A directory :envvar:`TRASHDIR` is created for setup, right before running :func:`setup` ().
#
#   Setup is not run if no tests are ever run
#**

#**
# .. envvar:: TRASHDIR
#
# Temporary directory where everything for the test file is stored
#
# Automatically generated and removed (unless :envvar:`TESTLIB_KEEP_TEMP_DIRS` is changed)
#
# .. seealso::
#   :envvar:`TESTDIR`
#**

#**
# .. envvar:: TESTDIR
#
# Unique temporary directory for a single test (in :envvar:`TRASHDIR`)
#
# Automatically generated and removed (unless :envvar:`TESTLIB_KEEP_TEMP_DIRS` is changed)
#
# .. seealso::
#   :envvar:`TRASHDIR`
#**

#**
# .. envvar:: TESTLIB_DIR
#
# Directory of testlib's source code
#**
: ${TESTLIB_DIR="${VSI_COMMON_DIR}/tests"}

#**
# .. function:: teardown
#
# Function run after the last test
#
# .. note::
#   Teardown is not run if no tests are ever run
#**

#**
# .. envvar:: TESTLIB_KEEP_TEMP_DIRS
#
# Keep the trashdir/setup dir
#
# Debug flag to keep the temporary directories generated when testing. Set to ``1`` to keep directories.
#
# :Default: ``0``
#**

#**
# .. envvar:: TESTLIB_KEEP_PAUSE_AFTER_ERROR
#
# Pauses before allowing testlib to cleanup and delete all temporary files.
#
# A better alternative to :envvar:`TESTLIB_KEEP_TEMP_DIRS`, so that you are given time to investigate a problem, and can then press any key to cleanup the temporary directory
#
# :Default: ``0``
#**

#**
# .. envvar:: TESTLIB_SHOW_TIMING
#
# Display test time after each test
#
# Debug flag to display time elapsed for each test. Set to ``1`` to enable.
#
# :Default: ``0``
#**

#**
# .. envvar:: TESTLIB_RUN_SINGLE_TEST
#
# Run a single test
#
# Instead of running all the tests in a test file, only the tests with a description exactly matching the value of :envvar:`TESTLIB_RUN_SINGLE_TEST` will be run. Useful for debugging a specific test/piece of code
#
# :Default: *unset*
#**

#**
# .. envvar:: TESTLIB_SKIP_TESTS
#
# A bash regex expressions that designates tests to be skipped.
#
# :Default: *unset*
#
# .. rubric:: Examples
#
# .. code-block:: bash
#
#    TESTLIB_SKIP_TESTS='^New Just$|foo'
#    # Skip "New Just" and anything with "foo" it is, e.g. "food"
#**

#**
# .. envvar:: TESTLIB_REDIRECT_OUTPUT
#
# Redirects stdout and stderr to temporary files
#
# By default, all tests are run with ``set -xv`` for debugging purposes when a tests fails. This output is stored in a out/err/xtrace file temporarily and only displayed if a tests fails. You can set this variable to control the streams to always output.
#
# :Values: * ``3`` Redirect stdout, stderr, and xtrace
#          * ``2`` Redirect stderr, and xtrace, but let stdout through
#          * ``1`` Redirect xtrace, but let stdout and stderr through. On bash 4.0 and older, this will let xtrace through too
#          * ``0`` Let everything through
#
# :Default: ``3``
#**

: ${TESTLIB_REDIRECT_OUTPUT=3}

#**
# .. envvar:: TESTLIB_PS4
#
# Optionally set a custom PS4 output for trace output on test errors. If unset, the testlib default is use: ``+${BASH_SOURCE[0]##*/}:${LINENO})\t``
#
# :Default: *unset*
#**

#**
# .. envvar:: TESTLIB_STOP_AFTER_FAILS
#
# If set, stops after this many tests have fails in a single file. Instead of running the rest of the tests in a file, they are skipped.
#
# :Default: ``0`` - Unlimited
#**

#**
# .. function:: atexit
#
# Function that runs at process exit
#
# .. rubric:: Usage
#
# Automatically called on exit by trap.
#
# Checks to see if :func:`teardown` is defined, and calls it. :func:`teardown` is typically a function, alias, or something that makes sense to call.
#**
atexit ()
{
  test_status="${?}"

  if [ "${TESTLIB_KEEP_TEMP_DIRS-}" != "1" ]; then
    # Only works if stdin is connected, aka only one test file on run_tests
    if [ "${failures}" != "0" -a "${TESTLIB_KEEP_PAUSE_AFTER_ERROR-0}" = "1" ]; then
      read -rsn1 -p "Press any key to continue..." x
    fi
    rm -rf "${TRASHDIR}"
  fi

  if [ "${tests}" -ne 0 ] && [ "${tracking_touched_files-}" = "1" ]; then
    cleanup_touched_files
  fi

  if [ "${__testlib_setup_failed-0}" = "1" ]; then
    echo "Calling setup() ${TESTLIB_BAD_COLOR}failed${TESTLIB_RESET_COLOR}!!!" >&2
    # TODO: Add trace/stderr/stdout printout here
    exit 2
  fi

  if [ -n "${test_description+set}" ]; then
    end_test "${test_status}"
    echo "${TESTLIB_BAD_COLOR}WARNING${TESTLIB_RESET_COLOR}: end_test not added at end of last test." 1>&2
  elif [ "${test_status}" != "0" ]; then
    echo "${TESTLIB_BAD_COLOR}ERROR${TESTLIB_RESET_COLOR} Test failed outside of test. Maybe a sourced file or the test file has a problem" 1>&2
    exit 123
  fi
  unset test_status

  if [ "${__testlib_ran_a_test-}" = "1" ] && \
     type -t teardown &> /dev/null && \
     [ "$(command -v teardown)" == "teardown" ]; then
    # In case there is a failure in teardown, add one to the failures so the
    # summary notes this, and at least you'll which file is failing
    if [ -d "${TESTLIB_SUMMARY_DIR-}" ]; then
      echo "${tests} $((failures+1)) ${unexpected_successes} ${expected_failures} ${required_failures} ${skipped}" > "${TESTLIB_SUMMARY_DIR}/$(basename "${0}")"
    fi

    trap "__testlib_teardown_failed=1; atexit_part2" ERR
    set_flag E
    teardown
    reset_flag E
    trap -- ERR
  fi

  atexit_part2
}

function atexit_part2()
{
  if [ "${__testlib_teardown_failed-0}" = "1" ]; then
    echo "Calling teardown() ${TESTLIB_BAD_COLOR}failed${TESTLIB_RESET_COLOR}!!!" >&2
    # TODO: Add trace/stderr/stdout printout here
  fi

  local BOLD_COLOR
  if [ "${failures}" -eq "0" ]; then
    BOLD_COLOR="${TESTLIB_BOLD_COLOR}"
  else
    BOLD_COLOR="${TESTLIB_BAD_COLOR}"
  fi

  printf "%s summary: %d tests, ${BOLD_COLOR}%d failures${TESTLIB_RESET_COLOR}, %d unexpected successes, %d expected failures, %d required failures, %d skipped\n" \
         "${0}" "${tests}" "${failures}" "${unexpected_successes}" "${expected_failures}" "${required_failures}" "${skipped}"

  if [ -d "${TESTLIB_SUMMARY_DIR-}" ]; then
    echo "${tests} ${failures} ${unexpected_successes} ${expected_failures} ${required_failures} ${skipped}" > "${TESTLIB_SUMMARY_DIR}/$(basename "${0}")"
  fi

  if [ "${failures}" -gt "0" ]; then
    # Make the exit code 123, to be consistent with xargs' behavior
    exit 123
  elif [ "${__testlib_teardown_failed-0}" = "1" ]; then
    exit 3
  else
    exit 0
  fi
}
trap_chain "atexit" EXIT


if [ -z "${TESTLIB_PS4+set}" ]; then
  if declare -p BASH_SOURCE &> /dev/null; then
    PS4=$'+\x1b[0m${TESTLIB_BAD_COLOR}${BASH_SOURCE[1]+${TESTLIB_RESET_COLOR}}${BASH_SOURCE[0]##*/}${TESTLIB_RESET_COLOR}:${LINENO}${FUNCNAME[0]:+:${FUNCNAME[0]}()}\t'
  else # Else sh probably
    # Not as accurate, but better than nothing
    PS4=$'+\x1b[0m${0##*/}:${LINENO})\t'
  fi
else
  PS4="${TESTLIB_PS4}"
fi

# Common code for begin tests
_begin_common_test ()
{
  TESTDIR="${TRASHDIR}/test${tests}"
  mkdir -p "${TESTDIR}"
  pushd "${TESTDIR}" &> /dev/null
  # This makes calling end_test between tests "optional", but highly recommended.
  # end_test does have to be called after the last test, especially if teardown
  # is defined after the last test
  if [ -n "${test_description+set}" ]; then
    end_test "${test_status}"
    echo "${TESTLIB_BAD_COLOR}WARNING${TESTLIB_RESET_COLOR}: end_test not added at end of a test." 1>&2
    declare -p BASH_SOURCE
  fi
  unset test_status

  # Set flag defaults that could be overridable in certain test types.
  # This needs to be after end_test call above in order to keep end_test
  # optional
  expected_failure="${_expected_failure-0}"
  required_fail="${_required_fail-0}"

  tests="$(( tests + 1 ))"
  local test_file_name="$(basename "${0}")"
  test_file_name="${test_file_name%.*}"
  test_file_name="${test_file_name#test-}"
  test_description="${test_file_name} - ${1}"

  if [ "${TESTLIB_RUN_SINGLE_TEST+set}" = "set" ] && \
     [ "${1}" != "${TESTLIB_RUN_SINGLE_TEST}" ]; then
    skip_next_test
  elif [ -n "${TESTLIB_SKIP_TESTS+set}" ] && [[ ${1} =~ ${TESTLIB_SKIP_TESTS} ]]; then
    skip_next_test
  fi

  # Run setup if this is the first test
  if [ "${__testlib_skip_test-}" != "1" ] && \
     [ "${__testlib_ran_a_test-}" != "1" ]; then
    if type -t setup &> /dev/null && \
       [ "$(command -v setup)" == "setup" ]; then

      trap "__testlib_setup_failed=1" ERR
      set_flag E
      setup
      reset_flag E
      # Don't need to backup traps, since this will not affect parent function's
      # traps
      trap -- ERR
    fi
    __testlib_ran_a_test=1
  fi

  # This has to run after setup, since setup is where track_touched_files should
  # be called
  if [ "${tracking_touched_files-}" = "1" ]; then
    track_touched_file="$(mktemp -u)"
    ttouch "${track_touched_file}"
  fi

  if [ "${TESTLIB_SHOW_TIMING-0}" = "1" ]; then
    __testlib_time_0="$(get_time_seconds)"
  fi

  if [ "${bash_feature_allocate_file_descriptor}" = "0" ]; then
    case ${TESTLIB_REDIRECT_OUTPUT} in
      3)
        exec {stdout}>&1 {stderr}>&2
        ;;
      2)
        exec {stderr}>&2
        ;;
    esac
    xtrace="${TESTDIR}/xtrace"
    # bash_feature_allocate_file_descriptor and bash_feature_xtracefd coincide,
    # so there's no need to test for bash_feature_xtracefd
    if [ "${TESTLIB_REDIRECT_OUTPUT}" -ge "1" ]; then
      if [ "${bash_bug_allocate_xtracefd}" = "0" ]; then
        local fd
        find_open_fd fd
        eval "exec ${fd}>\"${xtrace}\""
        # This needs to be set after opening the file, to avoid an error message
        BASH_XTRACEFD="${fd}"
      else
        exec {BASH_XTRACEFD}>"${xtrace}"
      fi
    fi
  else
    if [ "${TESTLIB_REDIRECT_OUTPUT}" -ge "3" ]; then
      find_open_fd stdout
      eval "exec ${stdout}>&1"
    fi
    if [ "${TESTLIB_REDIRECT_OUTPUT}" -ge "2" ]; then
      find_open_fd stderr
      eval "exec ${stderr}>&2"
    fi
    # Doesn't support BASH_XTRACEFD
    xtrace="${TESTDIR}/err"
  fi

  out="${TESTDIR}/out"
  err="${TESTDIR}/err"

  case "${TESTLIB_REDIRECT_OUTPUT}" in
    3)
      exec 1>"${out}" 2>"${err}"
      ;;
    2)
      exec 2>"${err}"
      ;;
  esac

  # Allow the subshell to exit non-zero without exiting this process
  set -x +e
}

#**
# .. function:: begin_test
#
# Beginning of test demarcation
#
# .. rubric:: Usage
#
# Mark the beginning of a test. A subshell should immediately follow this statement.
#
# .. seealso::
#   :func:`end_test`
#**
begin_test ()
{
  test_status="${?}" # Must be first command
  _begin_common_test ${@+"${@}"}
}

#**
# .. function:: begin_expected_fail_test
#
# Beginning of expected to fail test demarcation
#
# .. rubric:: Usage
#
# Define the beginning of a test that is expected to fail. Failures may only occur in "fail zones" denoted by :func:`begin_fail_zone` and :func:`end_fail_zone`. When a test fails in a fail zone, it is counted as a success. If a test that was expected to fail never fails, it counts as an "unexpected success" rather than a normal success. If the test fails outside a fail zone, it is marked as a failure.
#
# The typical use case for expecting a failure is when a known bug is being tested and has not or cannot be fixed yet. For this reason, a success is counted as an "unexpected success" rather than a normal success. While unexpected successes do not cause a non-zero exit code, they can easily be noticed as something that should be checked out.
#
# .. note::
#   :func:`end_fail_zone` is not typically needed.
#**
begin_expected_fail_test()
{
  test_status="${?}" # Must be first command
  # Override _begin_common_test default
  _expected_failure=1 _begin_common_test ${@+"${@}"}
}

#**
# .. function:: begin_required_fail_test
#
# Beginning of required to fail test demarcation
#
# .. rubric:: Usage
#
# Define the beginning of a test that is required to fail. Failures may only occur in "fail zones" denoted by :func:`begin_fail_zone` and :func:`end_fail_zone`. When a test fails in a fail zone, it is counted as a success. If a test that was required to fail never fails, it counts as a failure. If the test fails outside a fail zone, it is marked as a failure.
#
# The typical use case for requiring a failure is testing that an exception is raise under proper circumstances.
#
# .. note::
#   :func:`end_fail_zone` is not typically needed.
#**
begin_required_fail_test()
{
  test_status="${?}" # Must be first command
  # Override _begin_common_test default
  _required_fail=1 _begin_common_test ${@+"${@}"}
}

#**
# .. function:: begin_fail_zone
#
# Start a fail zone
#
# In practice, having a test that is expected or required to fail leads to the possibility of a test failing somewhere you don't expect it to. For this reason, fail zones must be denoted in order for :func:`begin_expected_fail_test` and :func:`begin_required_fail_test` tests to succeed, and the failures must only occur in a fail zone. If a failure happens outside the fail zone, the test will be marked as a failure with the message ``SHOULD HAVE FAILED ELSEWHERE``.
#
# Typically this will be called right before the last last line of a test
#
# .. rubric:: Example
#
# .. code-block:: bash
#
#   begin_expected_fail_test "Some test"
#   (
#     setup_test
#     # Failing here would result in failure
#     begin_fail_zone
#     false is ok here
#   )
#**
begin_fail_zone()
{
  echo 1 > "${TRASHDIR}/.testlib_failure_zone"
}

#**
# .. function:: end_fail_zone
#
# End a fail zone
#
# While not common, it might be possible to have a test that is likely to fail in one of many places; for this reason a fail zone can be turned off, before being turned on again.
#
# .. rubric:: Example
#
# .. code-block:: bash
#
#   begin_required_fail_test "Some test"
#   (
#     setup_test
#     true something
#
#     begin_fail_zone
#     maybe_false
#     end_fail_zone
#
#     true again
#
#     begin_fail_zone
#     false_if_other_was_not
#     # end_fail_zone # not required at the end of the test, but won't hurt
#   )
#   end_test
#**
end_fail_zone()
{
  echo 0 > "${TRASHDIR}/.testlib_failure_zone"
}

#**
# .. function:: setup_test
#
# Sets up the test
#
# Once inside the () subshell, typically set -eu needs to be run, then other things such as checking to see if a test should be skipped, etc. need to be done. This is all encapsulated into :func:`setup_test`. This is required; without it, :func:`end_test` will know you forgot to call this and fail.
#
# This is also the second part of creating a skippable test.
#
# You are free to change "set -eu" after :func:`setup_test`, should you wish.
#
# .. rubric:: Usage
#
# Place at the beginning of a test
#
# .. rubric:: Example
#
# .. code-block:: bash
#
#   skip_next_test
#   begin_test "Skipping test"
#   (
#     setup_test
#     #test code here
#   )
#
# .. seealso::
#   :func:`skip_next_test`
#**
setup_test()
{
  # Identify that setup_test was called
  touch "${TRASHDIR}/.setup_test"

  # Check to see if this test should be skipped
  if [ "${__testlib_skip_test-}" = "1" ]; then
    exit 0
  fi

  set -eu
}

#**
# .. function:: end_test
#
# End of a test demarcation
#
# .. rubric:: Usage
#
# Mark the end of a test. Must be the first command after the test group, or else the return value will not be captured successfully.
#
# .. seealso::
#   :func:`begin_test`
#**
end_test()
{
  test_status="${1:-${?}}" # This MUST be the first line of this function
  set +x -e
  case "${TESTLIB_REDIRECT_OUTPUT}" in
    3) # Tested, this apparently works on bash 3.2
      exec 1>&${stdout} 2>&${stderr}
      ;;
    2)
      exec 2>&${stderr}
      ;;
  esac
  if [ "${TESTLIB_REDIRECT_OUTPUT}" -ge "1" -a "${xtrace}" != "${err}" ]; then
    # if these are different files, close xtrace, should only be here in bash
    # 4.1 or newer, so no need to waste time checking.
    exec {BASH_XTRACEFD}>&-
  fi
  popd &> /dev/null

  local time_e=''
  if [ "${TESTLIB_SHOW_TIMING-0}" = "1" ]; then
    time_e="$(awk "BEGIN {print \"\t\" $(get_time_seconds)-${__testlib_time_0}}")"
  fi

  # Handle missing call to setup_test
  if [ ! -e "${TRASHDIR}/.setup_test" ]; then
    # This is a "no matter what, failure". No expected/required failure work
    # around for this
    printf "%-80s ${TESTLIB_BAD_COLOR}SETUP FAILURE${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
    failures="$(( failures + 1 ))"
  # Handle a skipped test
  elif [ "${__testlib_skip_test-}" = "1" ] && [ "${test_status}" -eq "0" ]; then
    printf "%-80s ${TESTLIB_GOOD_COLOR}SKIPPED${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
    skipped="$((skipped+1))"
  # Handle a required fail
  elif [ "${required_fail}" -eq "1" ]; then
    if [ -f "${TRASHDIR}/.testlib_failure_zone" ]; then
      local required_failure_state="$(<"${TRASHDIR}/.testlib_failure_zone")"
      if [ "${test_status}" != "0" ]; then
        if [ "${required_failure_state}" = "1" ]; then
          printf "%-80s ${TESTLIB_GOOD_COLOR}FAIL REQUIRED${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
          required_failures="$((required_failures+1))"
        else
          printf "%-80s ${TESTLIB_BAD_COLOR}SHOULD HAVE FAILED ELSEWHERE${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
          failures="$(( failures + 1 ))"
          testlib_print_test_trace
        fi
      else
        failures="$(( failures + 1 ))"
        printf "%-80s ${TESTLIB_BAD_COLOR}SHOULD HAVE FAILED${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
        testlib_print_test_trace
      fi
    else
      # Handle missing call to begin_fail_zone
      printf "%-80s ${TESTLIB_BAD_COLOR}REQUIRED FAILURE SETUP ERROR${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
      failures="$(( failures + 1 ))"
    fi
  # Handle an expected fail
  elif [ "${expected_failure}" -eq "1" ]; then
    if [ -f "${TRASHDIR}/.testlib_failure_zone" ]; then
      local expected_failure_state="$(<"${TRASHDIR}/.testlib_failure_zone")"
      if [ "${test_status}" != "0" ]; then
        if [ "${expected_failure_state}" = "1" ]; then
          printf "%-80s ${TESTLIB_GOOD_COLOR}FAIL EXPECTED${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
          expected_failures="$(( expected_failures + 1 ))"
        else
          printf "%-80s ${TESTLIB_BAD_COLOR}SHOULD HAVE FAILED ELSEWHERE${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
          failures="$(( failures + 1 ))"
          testlib_print_test_trace
        fi
      else
        printf "%-80s ${TESTLIB_WARN_COLOR}UNEXPECTED SUCCESS${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
        unexpected_successes="$(( unexpected_successes + 1 ))"
      fi
    else
      # Handle missing call to begin_fail_zone
      printf "%-80s ${TESTLIB_BAD_COLOR}EXPECTED FAILURE SETUP ERROR${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
      failures="$(( failures + 1 ))"
    fi
    # Remove account file
    rm -f "${TRASHDIR}/.testlib_failure_zone"
  elif [ "${required_fail}" -eq 0 ] && [ "${test_status}" -eq "0" ]; then
    printf "%-80s ${TESTLIB_GOOD_COLOR}OK${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
  else
    printf "%-80s ${TESTLIB_BAD_COLOR}FAILED${TESTLIB_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
    failures="$(( failures + 1 ))"
    testlib_print_test_trace
  fi

  if [ "${tracking_touched_files-}" = "1" ]; then
    cleanup_touched_files
  fi

  unset test_description
  unset __testlib_skip_test

  rm "${TRASHDIR}/.setup_test" || :

  if [ "${TESTLIB_STOP_AFTER_FAILS-0}" != "0" ] && [ "${failures}" -ge "${TESTLIB_STOP_AFTER_FAILS}" ]; then
    # Skip the rest of the tests.
    TESTLIB_SKIP_TESTS='.*'
  fi
}

testlib_print_test_trace()
{
  local test_output
  # Darling has issue printing out too fast https://github.com/darlinghq/darling/issues/640
  # This perl command can slow it down enough that it works on my computer
  if command -v sw_vers &> /dev/null && [ "$(sw_vers -buildVersion)" = "Darling" ]; then
    find_open_fd test_output
    # Technically, the process substitution happens before the eval, but that
    # doesn't matter in the end, still works correctly.
    eval "exec ${test_output}> >(perl -e 'print && select undef,undef,undef,0.0001 while <>;' >&2 )"
  elif [ "${bash_feature_allocate_file_descriptor}" = "0" ]; then
    exec {test_output}>&2
  else
    find_open_fd test_output
    eval "exec ${test_output}>&2"
  fi

  (
    if [ "${TESTLIB_REDIRECT_OUTPUT}" -ge "1" ]; then

      if ! command -v column &> /dev/null; then
        function column()
        {
          cat -
        }
      fi

      if [ "${err}" = "${xtrace}" ]; then
        echo "-- stderr --"
      else
        echo "-- xtrace --"
      fi
      grep -v -e $'^\+[^\t]*\tend_test' \
              -e $'^\+[^\t]*\tset +x -e' <"${xtrace}" |
        sed $'s|^[^+]| \t&|' |
        column -c1 -s $'\t' -t |
        sed 's/^/    /'
    fi
    if [ "${TESTLIB_REDIRECT_OUTPUT}" -ge "2" ]; then
      if [ "${err}" != "${xtrace}" ]; then
        echo "-- stderr --"
        sed 's/^/    /' <"${err}"
      fi
    fi
    if [ "${TESTLIB_REDIRECT_OUTPUT}" -ge "3" ]; then
      echo "-- stdout --"
      sed 's/^/    /' <"${out}"
    fi
    echo "-- EOF "${test_description}" --"
  ) 1>&${test_output}

  # Close fd test_output, which will end perl if its running
  if [ "${bash_feature_allocate_file_descriptor}" = "0" ]; then
    exec {test_output}>&-
  else
    eval "exec ${test_output}>&-"
  fi
}

#**
# .. function:: skip_next_test
#
# Function to indicate the next test should be skipped
#
# This is the first part of creating a skippable test, used in conjunction with :func:`setup_test`
#
# .. rubric:: Example
#
# .. code-block:: bash
#
#   For example, skip if docker command not found
#
#     if ! command -v docker &> /dev/null; then
#       skip_next_test
#     fi
#     begin_test "My test"
#     (
#       setup_test
#       [ "$(docker run -it --rm ubuntu:14.04 echo hi)" = "hi" ]
#     )
#
# .. seealso::
#   :func:`setup_test`
#
# .. note::
#   This must be done outside of the test, or else the skip variable will not be set and detected by :func:`end_test`
#**
skip_next_test()
{
  __testlib_skip_test=1
}

#**
# .. function:: track_touched_files
#
# Start tracking touched files
#
# After running :func:`track_touched_files`, any call to :func:`ttouch` will cause that file to be added to the internal list (touched_files). Just prior to the teardown phase, all of these files will be automatically removed for your convenience.
#
# :func:`ttouch` should be used in cases where a file cannot be redirected to :envvar:`TESTDIR` or :envvar:`TRASHDIR`
#
# .. rubric:: Example
#
# .. code-block:: bash
#
#   setup()
#   {
#     track_touched_files
#   }
#   begin_test Testing
#   (
#     ttouch /tmp/hiya
#   )
#   end_test
#
# .. rubric:: Usage
#
# Should be called before the :func:`begin_test` block, not inside. Inside a () subshell block will not work. Setup is the logical place to put it.
#
# .. rubric:: Bugs
#
# Does not work in sh, only ``bash``. Uses array, and I didn't want to make this use a string instead.
#
# Not thread safe. Use a different file for each thread
#
# .. seealso::
#   :func:`cleanup_touched_files`
#**
track_touched_files()
{
  tracking_touched_files=1
}

#**
# .. function:: ttouch
#
# Touch function that should behave like the original touch command
#
# .. seealso::
#   :func:`track_touched_files`
#**
ttouch()
{
  local filename
  local end_options=0

  touch "${@}"

  # Skip all options
  while [ "${#}" -gt "0" ]; do
    if [ "${end_options}" = "0" ] && [ "${1}" = "--" ]; then
      end_options=1
      shift 1
    elif [ "${end_options}" = "0" ] && [ "${#1}" -gt "0" ] && [ "${1:0:1}" = "-" ]; then
      shift 1
    else
      filename="${1}"
      # force to be absolute path
      if [ "${filename:0:1}" != "/" ]; then
        filename="$(pwd)/${1}"
      fi
      printf "${filename}\0" >> "${track_touched_file}"
      shift 1
    fi
  done
}

#**
# .. function:: cleanup_touched_files
#
# Delete all the touched files
#
# At the end of the last test, delete all the files in the array
#**
cleanup_touched_files()
{
  local touched_file
  local touched_files
  local line

  # This will normally get called an extra time at exit, so the existence of
  # the file acts as a check.
  if [ -f "${track_touched_file}" ]; then
    while IFS='' read -r -d '' touched_file 2>/dev/null || [ -n "${touched_file}" ]; do
      if [ -e "${touched_file}" ] && [ ! -d "${touched_file}" ]; then
        \rm "${touched_file}"
      fi

      touched_file='' #Have to clear it, in case the timeout times out
    done < "${track_touched_file}"
  fi
}