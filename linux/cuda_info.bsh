#!/usr/bin/env false bash

if [[ ${-} != *i* ]]; then
  source_once &> /dev/null && return 0
fi

if [ -z "${VSI_COMMON_DIR+set}" ]; then
  VSI_COMMON_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.."; pwd)"
fi

source "${VSI_COMMON_DIR}/linux/web_tools.bsh"
source "${VSI_COMMON_DIR}/linux/compat.bsh"
source "${VSI_COMMON_DIR}/linux/aliases.bsh"
source "${VSI_COMMON_DIR}/linux/versions.bsh"
source "${VSI_COMMON_DIR}/linux/requirements.bsh"

#*# linux/cuda_info

#**
# ================
# CUDA Information
# ================
#
# .. default-domain:: bash
#
# .. file:: cuda_info.bsh
#
# Determine easy to use capabilities for CUDA devices
#
# There are many versions of CUDA, CUDA card architectures, etc... Knowing how to compile for a specific card is hard enough, but it's very difficult to know what the right architectures are for your specific card and what the limitations are based on your version of CUDA/NVIDIA driver, etc. This script should help determine what versions you have and suggest what architectures to use. This is good enough for an automated solution to get up and running, but these suggestions are not absolute. You may find a fine-tuned configuration that works better on a case-by-case basis.
#
#
# A number of variables are set by the following functions.
#
# .. var:: CUDA_VERSION
#
# The version of CUDA being used. These functions will attempt to discover the CUDA Toolkit in commonly known locations and gather a list of all discovered CUDAs in the sorted array :var:`CUDA_VERSIONS`. Then, the highest capable version of CUDA is picked and set to :var:`CUDA_VERSION`.
#
# :var:`CUDA_VERSION` can optionally be set to a specific version (e.g. "7.5.13"), in which case other CUDA versions will not be discovered and :var:`CUDA_VERSIONS` will not be populated.
#
# .. note::
#
#    Currently, CUDA Toolkits are discovered by checking the system ``PATH`` and ``/usr/local/cuda*/bin/`` directories for the nvcc executable. More paths should be added to this file as they become necessary.
#
# .. var:: CUDA_VERSIONS
#
# List of all :var:`CUDA_VERSION`-s discovered
#
# .. var:: CUDA_ARCHES
#
# An array of CUDA "virtual" instruction sets supported by CUDA. Every version of CUDA (``nvcc``) has a set of "virtual" ``compute_xx`` architectures (ISAs) that it can build against when compiling code for "real" ``sm_xx`` architectures.
#
# This array contains the list of the compute (virtual) architectures supported by the :var:`CUDA_VERSION` version of CUDA as an array of two digit numbers.
#
# .. rubric:: Example
#
# .. code-block:: bash
#
#    $ echo "${CUDA_ARCHES[@]}"
#    20 30 32 35 37 50 52 53 60 61 62
#
#    Adding the periods to the architecture version number:
#
#    y=()
#    for x in ${CUDA_ARCHES[@]+"${CUDA_ARCHES[@]}"}; do
#      y+=("${x:0:${#x}-1}.${x:${#x}-1:1}")
#    done
#
#    $ echo "${y[@]}"
#    2.0 3.0 3.2 3.5 3.7 5.0 5.2 5.3 6.0 6.1 6.2
#
# .. seealso::
#   :var:`CUDA_DEPRECATED`
#
# .. var:: CUDA_CODES
#
# An array of CUDA "real" instruction sets supported by CUDA. Every version of CUDA (``nvcc``) has a set of "real" ``sm_xx`` architectures that that it can assemble native (CUDA binary) code for.
#
# This array contains a list of the sm architectures supported by the :var:`CUDA_VERSION` version of CUDA as an array of two digit numbers.
#
# .. seealso::
#   :var:`CUDA_DEPRECATED`
#
# .. var:: CUDA_LTOS
#
# Starting in CUDA 11, NVIDIA introduced Link Time Optimizations (``lto_``) architectures in addition to the already existing ``sm_`` and ``compute_``
#
# Separately compiled kernels may not have as high of performance as if they were compiled together with the rest of the executable code because of the inability to inline code across files. Link-time optimization is a way to recover that improved performance
#
# .. var:: CUDA_MINIMAL_DRIVER_VERSION
#
# Every version of CUDA has a minimal version of the NVIDIA graphics-card driver that must be installed in order to support that version of CUDA. This was largely undocumented until CUDA 10 came out, despite being obviously important. This variable is set to the minimum required version of the NVIDIA driver for the :var:`CUDA_VERSION` version of CUDA, as best as we've been able to determine.
#
# .. var:: CUDA_COMPATIBLE_DRIVER_VERSION
#
# Starting in CUDA 11, NVIDIA introduced a ``cuda-compat` package that can be installed on machines with older NVIDIA Drivers for increased compatibility with these drivers. This is the minimum supported driver version that will handle the ``cuda-compat`` runtime.
#
# .. var:: CUDA_DEPRECATED
#
# Some versions of CUDA support old instruction sets, but print a deprecated warning. For those versions of CUDA, a :var:`CUDA_DEPRECATED` array is defined to list the two digit architectures that are supported yet deprecated in the :var:`CUDA_VERSION` version of CUDA.
#
# .. var:: CUDA_FORWARD_PTX
#
# PTX arch for newer CUDA cards. In situations where you are making portable fatbinaries, you should compile for every architecture. However, in order to future proof your fatbin for architectures newer than your current version of CUDA supports, you will need to compile to a pure virtual architecture using the PTX feature so that the real architecture can be JIT (Just-In-Time) compiled.
#
# :var:`CUDA_FORWARD_PTX` identifies the fullest-featured PTX architecture so that you can choose to add this to your builds.
#
# .. var:: CUDA_CARDS
#
# The names of the CUDA cards. Exact values may vary, e.g. "Titan X (Pascal)" vs "Titan Xp".
#
# .. var:: CUDA_CARD_FAMILIES
#
#   The family name of each card in :var:`CUDA_CARDS`.
#
# .. var:: CUDA_CARD_ARCHES
#
# The specific CUDA architecture a card natively supports, for each card in :var:`CUDA_CARDS`.
#
# .. var:: CUDA_SUGGESTED_ARCHES
#
# Suggested "virtual" architectures to compile for. Instead of compiling for every architecture that the :var:`CUDA_VERSION` version of CUDA supports, :var:`CUDA_SUGGESTED_ARCHES` is the intersection between :var:`CUDA_CARD_ARCHES` and :var:`CUDA_ARCHES` so that you compile only for your cards.
#
# .. var:: CUDA_SUGGESTED_CODES
#
# Suggested "real" architectures to compile for. Instead of compiling for every architecture that the :var:`CUDA_VERSION` version of CUDA supports, :var:`CUDA_SUGGESTED_CODES` is the intersection between :var:`CUDA_CARD_ARCHES` and :var:`CUDA_CODES` so that you compile only for your cards.
#
# .. var:: CUDA_SUGGESTED_PTX
#
# Suggested PTX architectures to compile for. If your graphics card is too new for the :var:`CUDA_VERSION` version of CUDA, you will need to compile to a pure virtual architecture (by embedding PTX code in the fatbinary) in order to use it. That way, the real architecture can be JIT (Just-In-Time) compiled for at runtime.
#
# :var:`CUDA_SUGGESTED_PTX` identifies the PTX architectures you need to run on newer (unrecognized) cards. You can choose to add them to your builds.
#**

#**
# .. function:: discover_cuda_versions
#
# :Output: :var:`cuda_info.bsh CUDA_VERSIONS`
#          :var:`cuda_info.bsh CUDA_VERSION`
#
# Find CUDA development kits
#
# .. note::
#
#    Will not work on macOS if it has NVIDIA and two or more versions of CUDA installed.
#**
function discover_cuda_versions()
{
  CUDA_VERSIONS=()
  # local IFS=$'\n' # Handle spaces, tabs, but not newlines in the paths
  local version
  for version in nvcc /usr/local/cuda*/bin/nvcc; do
    if ! command -v "${version}" &> /dev/null; then
      continue
    fi
    CUDA_VERSIONS+=($(NVCC="${version}" nvcc_version))
  done

  # Maybe nvcc isn't running right or is missing, check for version files
  if [ "${#CUDA_VERSIONS[@]}" = "0" ]; then
    for version in /usr/local/cuda*/version.txt; do
      if [ -r "${version}" ]; then
        CUDA_VERSIONS+=($(version_txt_cuda_version "${version}"))
      fi
    done
    for version in /usr/local/cuda*/version.json; do
      if [ -r "${version}" ]; then
        CUDA_VERSIONS+=($(version_json_cuda_version "${version}"))
      fi
    done
  fi

  if [ "${#CUDA_VERSIONS[@]}" = "0" ] && command -v "${NVIDIA_SMI}" &> /dev/null; then
    echo "No CUDA found: Determining max supported CUDA instead" >&2
    CUDA_VERSIONS=($(nvidia_smi_cuda_version))
  fi

  # Prevent macOS from crashing. Not sure why this is new. Might be darling
  if [ "${#CUDA_VERSIONS[@]}" -gt "1" ]; then
    CUDA_VERSIONS=($(sort -uV <<< ${CUDA_VERSIONS[*]+"${CUDA_VERSIONS[*]}"}))
  fi

  # Get the highest cuda level
  CUDA_VERSION="${CUDA_VERSIONS[@]+"${CUDA_VERSIONS[${#CUDA_VERSIONS[@]}-1]}"}"
}

#**
# .. function:: nvidia_smi_cuda_version
#
# Starting at NVIDIA Driver version 410.72, ``nvidia-smi`` started listing the maximum version of CUDA that driver supports. This function can be used to parse that.
#
# :Parameters: * ``NVIDIA_SMI`` - Optional path to a specific ``nvidia-smi``. Default: ``nvidia-smi`` (using the system path)
#
# :Output: *stdout* - echoes out the version of CUDA that ``nvidia-smi`` says it supports. If this version of ``nvidia-smi`` does not say, nothing is output.
#**
function nvidia_smi_cuda_version()
{
  ${NVIDIA_SMI} | sed -n${sed_flag_rE} 's|.*CUDA Version: *([0-9.]*).*|\1|;
                                        t done
                                        b
                                        :done
                                        p;
                                        q;'
}

#**
# .. function:: device_query_cuda_capability
#
# :Output: * :var:`cuda_info.bsh CUDA_CARD_FAMILIES`
#          * :var:`cuda_info.bsh CUDA_CARD_ARCHES`
#          * :var:`cuda_info.bsh CUDA_CARDS`
#
# :Parameters: * ``DEVICE_QUERY`` - Optional path to a specific ``deviceQuery``. Default: ``deviceQuery`` (using the system path)
#
# :Return Value: * ``0`` - No errors
#                * Non-zero - If the output of ``deviceQuery`` does not contain any CUDA capabilities, then something likely failed. This is usually caused by running a contianer without the NVIDIA extensions or having an insufficient NVIDIA driver for the version of CUDA ``deviceQuery`` is compiled for.
#
# Device query is one of the sample programs in the CUDA Samples that prints out useful information about the connected CUDA devices.
#
# The ``deviceQuery`` executable is compiled from the source code typically found the in ``/usr/local/cuda/samples/1_Utilities/deviceQuery/``, but can be downloaded precompiled for Linux from https://goo.gl/equvX3
#**
function device_query_cuda_capability()
{
  local card_info

  # deviceQuery may fail, for example, if running without nvidia-docker or if the CUDA driver version is insufficient for CUDA runtime version
  card_info="$(${DEVICE_QUERY-deviceQuery} | \grep -E "CUDA Capability Major/Minor version number|^Device")" || return ${?}
  CUDA_CARD_ARCHES=($(echo "${card_info}" | \awk '/CUDA Capability Major\/Minor version number/{sub(/\./, "", $NF); print $NF}'))
  cuda_arch_to_cuda_family

  local IFS=$'\n'
  CUDA_CARDS=($(echo "${card_info}" | \awk '/^Device/{$1=$2=""; $0=$0; $1=$1; print}' | \sed 's|"||g'))
}

#**
# .. function:: nvidia_smi_cuda_capability
#
# Starting at NVIDIA Driver version >500, ``nvidia-smi`` provides the ``compute_cap`` query property. This function can be used to parse that.
#
# :Output: * :var:`cuda_info.bsh CUDA_CARD_FAMILIES`
#          * :var:`cuda_info.bsh CUDA_CARD_ARCHES`
#          * :var:`cuda_info.bsh CUDA_CARDS`
#
# :Parameters: * ``NVIDIA_SMI`` - Optional path to a specific ``nvidia-smi``. Default: ``nvidia-smi`` (using the system path)
#
# :Return Value: * ``0`` - No errors
#                * Non-zero - If the output of ``nvidia-smi`` does not contain any CUDA cards, then something likely failed.
#**
function nvidia_smi_cuda_capability()
{
  local card_info
  card_info="$(${NVIDIA_SMI-nvidia-smi} --query-gpu=gpu_name,compute_cap --format=csv,noheader)" || return ${?}

  local IFS=$'\n'
  CUDA_CARDS=( $(echo "${card_info}" | \awk -F', ' '{print $1}') )

  # remove dots from nvidia-smi architectures (e.g., "7.5"->"75")
  CUDA_CARD_ARCHES=( $(echo "${card_info}" | \awk -F', ' '{print $2}' | \sed 's/\.//g') )
  cuda_arch_to_cuda_family
}

#**
# .. function:: wmic_cuda_capability
#
# :Output: * :var:`cuda_info.bsh CUDA_CARDS`
#
# :Parameters: * ``WMIC`` - Optional path to a specific ``wmic``. Default: ``wmic.exe`` (using the system path)
#
# :Return Value: * ``0`` - No errors
#                * Non-zero - If the output of ``wmic.exe`` does not contain any CUDA cards, then something likely failed.
#**
function wmic_cuda_capability()
{
  local card_info
  card_info="$(${WMIC-wmic.exe} PATH win32_VideoController GET AdapterCompatibility,Name /format:csv)"  || return ${?}

  # replace all /r with /n
  card_info=$(echo "${card_info}" | sed 's/\r/\n/g')

  # line format is "Node,AdapterCompatibility,Name", e.g., "DUMMY,NVIDIA,NVIDIA GeForce GTX 1650 Ti"
  local IFS=$'\n'
  CUDA_CARDS=( $(echo "${card_info}" | \awk -F',' '$2=="NVIDIA" {print $3}') )
}

#**
# .. function:: nvidia_docker_plugin_cuda_capability
#
# The deprecated nvidia-docker v1 API actually hosted GPU information, which was useful for determining the CUDA Arches. This is rarely used now.
#
# :Parameters: [``NV_HOST``] - Environment variable to optionally specify a custom NVIDIA host for the GPUs. Default: ``http://localhost:3476``
#
# :Output: * :var:`cuda_info.bsh CUDA_CARD_FAMILIES`
#          * :var:`cuda_info.bsh CUDA_CARD_ARCHES`
#          * :var:`cuda_info.bsh CUDA_CARDS`
#**
function nvidia_docker_plugin_cuda_capability()
{
  local card_info="$(download_to_stdout ${NV_HOST-http://localhost:3476}/gpu/info)"

  CUDA_CARD_FAMILIES=($(echo "${card_info}" | \awk '/Family:/{print $NF}'))
  CUDA_CARD_ARCHES=($(echo "${card_info}" | \awk '/Arch:/{sub(/\./, "", $NF); print $NF}'))
  local IFS=$'\n'
  CUDA_CARDS=($(echo "${card_info}" | \awk '/Model:/{$1=""; $0=$0; $1=$1; print}'))
}

#**
# .. function:: cuda_arch_to_cuda_family
#
# Determines CUDA Family names based off of CUDA Arch stored in ``CUDA_CARD_ARCHES``
#
# :Parameters: :var:`cuda_info.bsh CUDA_CARD_ARCHES`
#
# :Output: :var:`cuda_info.bsh CUDA_CARD_FAMILIES`
#**
function cuda_arch_to_cuda_family()
{
  local cuda_card_arches cuda_card_families

  # Put in descending order
  cuda_card_arches=(  90     89  80     75     70    60     50      30     20    10) # Lowest possible architecture for a specific family
  cuda_card_families=(Hopper Ada Ampere Turing Volta Pascal Maxwell Kepler Fermi Tesla) # Matching Family name
  # Similar to:
  # https://github.com/pytorch/pytorch/blob/04024926f4d6ee07bfd2368a96d848879c982314/cmake/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmake#L192-L223
  CUDA_CARD_FAMILIES=()
  local arch fam ver
  for arch in ${CUDA_CARD_ARCHES[@]+"${CUDA_CARD_ARCHES[@]}"}; do
    for ver in "${!cuda_card_arches[@]}"; do
      if [ "${arch}" -ge "${cuda_card_arches[ver]}" ]; then
        CUDA_CARD_FAMILIES+=("${cuda_card_families[ver]}")
        continue 2
      fi
    done
    CUDA_CARD_FAMILIES+=("Unknown family")
  done
}

if command -v pgrep &> /dev/null; then
  function __nvidia_docker_is_running()
  {
    if \pgrep -f "${1}" &> /dev/null; then
      return 0
    fi
    return 1
  }
else
  function __nvidia_docker_is_running()
  {
    if [ "$(\ps -ef | \grep "${1}" | \wc -l)" -gt "1" ]; then
      return 0
    fi
    return 1
  }
fi

#**
# .. function:: discover_cuda_info
#
# Get CUDA info about each card
#
# :Output: * :var:`cuda_info.bsh CUDA_CARDS`
#          * :var:`cuda_info.bsh CUDA_CARD_ARCHES`
#          * :var:`cuda_info.bsh CUDA_CARD_FAMILIES`
#**
function discover_cuda_info()
{
  local IFS="${IFS}"
  local OLD_IFS="${IFS}"

  CUDA_CARD_FAMILIES=()

  # Attempt to use ``nvidia-smi --query-gpus=compute_cap ...``
  if nvidia_smi_cuda_capability &> /dev/null; then
    :
  # Attempt to use deviceQuery
  elif command -v "${DEVICE_QUERY-deviceQuery}" &> /dev/null && device_query_cuda_capability; then
    :
  # Else attempt to use the nvidia-docker daemon
  elif __nvidia_docker_is_running nvidia-docker-plugin || declare -p NV_HOST &> /dev/null; then
    nvidia_docker_plugin_cuda_capability
  else
    echo "nvidia-smi not available or does not have 'compute_cap' query, "
    echo "deviceQuery not found, and nvidia-docker v1 plugin not running."
    echo "deviceQuery can be downloaded from https://www.vsi-ri.com/bin/deviceQuery"
    echo "or https://goo.gl/equvX3"
    echo "(Use https://www.vsi-ri.com/bin/deviceQuery.exe for Windows)"
  fi
}

#**
# .. function:: cuda_capabilities
#
# :Parameters: ``$1`` - The CUDA version to check against. This is typically the ``nvcc`` version, but the docs version should work too. In a pinch, CUDA version will work, to some extent.
#
# :Output: * :var:`cuda_info.bsh CUDA_ARCHES`
#          * :var:`cuda_info.bsh CUDA_CODES`
#          * :var:`cuda_info.bsh CUDA_LTOS` - Starting in CUDA 11, Link-Time Optimization architectures were added
#          * :var:`cuda_info.bsh CUDA_MINIMAL_DRIVER_VERSION` - Minimal required NVIDIA Driver needed to run this version of CUDA
#          * :var:`cuda_info.bsh CUDA_COMPATIBLE_DRIVER_VERSION` - Starting in CUDA 10, you can install a ``cuda-compat`` package that supports a different version of the CUDA library that runs on older drivers up to that version
#          * :var:`cuda_info.bsh CUDA_DEPRECATED`
#
# Determine compiler capabilities for specific CDK
#**
function cuda_capabilities()
{
  # This is the most complete list of compute and sm architectures supported by
  # CUDA version that I know of. It was created using strings on nvcc from each
  # version of CUDA. Documentation, stackoverflow, and random blogs often miss
  # at least one piece of information and is not this complete nor will it
  # verify this table.

  CUDA_DEPRECATED=()
  CUDA_LTOS=()
  CUDA_CODES=()

  case "${1}" in
    1.*)
      CUDA_ARCHES=(10 11)
      ;;

    2.*)
      CUDA_ARCHES=(10 11 12 13)
      ;;

    3.0*)
      CUDA_ARCHES=(10 11 12 13 20)
      ;;
    3.1*)
      CUDA_ARCHES=(10 11 12 13 20 30)
      CUDA_CODES=(10 11 12 13 20 21 22 23 30)
      ;;
    3.2*|4.1*) # In 4.1, 22 and 23 were removed
      CUDA_ARCHES=(10 11 12 13 20)
      CUDA_CODES=(10 11 12 13 20 21)
      ;;
    4.0*)
      CUDA_ARCHES=(10 11 12 13 20)
      CUDA_CODES=(10 11 12 13 20 21 22 23)
      ;;
    4.2*)
      CUDA_ARCHES=(10 11 12 13 20 30)
      CUDA_CODES=(10 11 12 13 20 21 30)
      ;;

    5.*)
      CUDA_ARCHES=(10 11 12 13 20 30 35)
      CUDA_CODES=(10 11 12 13 20 21 30 35)
      ;;

    6.0*)
      CUDA_ARCHES=(10 11 12 13 20 30 32 35 50)
      CUDA_CODES=(10 11 12 13 20 21 30 32 35 50)
      CUDA_DEPRECATED=(10)
      ;;
    6.5*)
      CUDA_ARCHES=(11 12 13 20 30 32 35 37 50 52)
      CUDA_CODES=(11 12 13 20 21 30 32 35 37 50 52)
      CUDA_DEPRECATED=(11 12 13)
      ;;

    7.*)
      CUDA_ARCHES=(20 30 32 35 37 50 52 53)
      CUDA_CODES=(20 21 30 32 35 37 50 52 53)
      ;;

    8.*)
      CUDA_ARCHES=(20 30 32 35 37 50 52 53 60 61 62)
      CUDA_CODES=(20 21 30 32 35 37 50 52 53 60 61 62)
      CUDA_DEPRECATED=(20 21)
      ;;

    9.0*)
      CUDA_ARCHES=(30 32 35 37 50 52 53 60 61 62 70)
      ;;
    9.[1-2]*)
      CUDA_ARCHES=(30 32 35 37 50 52 53 60 61 62 70 72)
      ;;

    10.2*)
      CUDA_ARCHES=(30 32 35 37 50 52 53 60 61 62 70 72 75)
      CUDA_DEPRECATED=(30 32 35 37 50)
      ;;
    10.*)
      CUDA_ARCHES=(30 32 35 37 50 52 53 60 61 62 70 72 75)
      ;;

    # RC 11.0.167 technically allowed sm/compute 30, but that didn't make it to release
    11.0*)
      CUDA_ARCHES=(35 37 50 52 53 60 61 62 70 72 75 80)
      CUDA_DEPRECATED=(35 37 50)
      ;;
    11.[1-3].*|11.[1-3]) # Do it this way in case there is 11.10
      CUDA_ARCHES=(35 37 50 52 53 60 61 62 70 72 75 80 86)
      CUDA_DEPRECATED=(35 37 50)
      ;;
    11.4*)
      CUDA_ARCHES=(35 37 50 52 53 60 61 62 70 72 75 80 87 86) # Added undocumented sm_87
      CUDA_DEPRECATED=(35 37 50)
      ;;
    11.[5-7]*)
      CUDA_ARCHES=(35 37 50 52 53 60 61 62 70 72 75 80 86 87)
      CUDA_DEPRECATED=(35 37 50)
      ;;
    11.8*)
      CUDA_ARCHES=(35 37 50 52 53 60 61 62 70 72 75 80 86 87 89 90)
      CUDA_DEPRECATED=(35 37 50)
      ;;

    12.*) # 12.0-12.6
      # "SM90a or SM_90a, compute_90a – (for PTX ISA version 8.0) – adds acceleration for features like wgmma and setmaxnreg. This is required for NVIDIA CUTLASS"
      CUDA_ARCHES=(50 52 53 60 61 62 70 72 75 80 86 87 89 90a 90)
      # The 30s are gone, and there's no longer any deprecated? (50 was undeprecated?)
      CUDA_DEPRECATED=()
      ;;

    # 12.7*) ?
    #   # For a while now, nvcc --help appears to have the complete list of arches finally.
    #   echo -n 'CUDA_ARCHES=('; nvcc --help | sed -n '/^--gpu-architecture\|^--gpu-code/,/^ *$/p' | sed -n '/^ *Allowed/,/^$/p' | tr ',' '\n' | sed -E "s| *'(.*)'\\.?|\\1|; /^$/d" | grep ^sm | sort -u | sed 's|sm_||' | tr '\n' ' ' | sed 's| *$||'; echo ')'; echo -n 'CUDA_CODES=('; nvcc --help | sed -n '/^--gpu-architecture\|^--gpu-code/,/^ *$/p' | sed -n '/^ *Allowed/,/^$/p' | tr ',' '\n' | sed -E "s| *'(.*)'\\.?|\\1|; /^$/d" | grep ^compute | sort -u | sed 's|compute_||' | tr '\n' ' ' | sed 's| *$||'; echo ')'; echo -n 'CUDA_LTOS=('; nvcc --help | sed -n '/^--gpu-architecture\|^--gpu-code/,/^ *$/p' | sed -n '/^ *Allowed/,/^$/p' | tr ',' '\n' | sed -E "s| *'(.*)'\\.?|\\1|; /^$/d" | grep ^lto | sort -u | sed 's|lto_||' | tr '\n' ' ' | sed 's| *$||'; echo ')'
    #
    #   ### Old method, still works ###
    #   # run "strings `which nvcc` | grep compute_" (ARCHES) and "sm_" (CODES) and you'll get a complete list
    #   # The strings output typically has a "are deprecated and may be removed in a future release" message
    #   #
    #   # strings `command -v nvcc` | \grep '^sm_' | sed 's|sm_||' | tr '\n' ' '; echo
    #   # strings `command -v nvcc` | \grep '^compute_' | sed 's|compute_||' | tr '\n' ' '; echo
    #   # strings `command -v nvcc` | \grep '^lto_' | sed 's|lto_||' | tr '\n' ' '; echo
    #
    #   CUDA_ARCHES=(?)
    #   CUDA_CODES=(?) # only if different, else skip this
    #   CUDA_LTOS=(?) # only if different, else skip this
    #   # strings `command -v nvcc` | \grep '\(sm\|compute\).*deprecate'
    #   CUDA_DEPRECATED=(?)

    *)
      echo "CUDA Version ${CUDA_VERSION} is not programmed in here yet..." >&2
      CUDA_ARCHES=()
      CUDA_CODES=()
      ;;
  esac

  if [ "${#CUDA_CODES[@]}" = "0" ]; then
    CUDA_CODES=("${CUDA_ARCHES[@]}")
  fi

  # Starting in 11.0, lto architectures were added
  if [[ ${1} = 11.* ]] && [ "${#CUDA_LTOS[@]}" = "0" ]; then
    CUDA_LTOS=("${CUDA_ARCHES[@]}")
  fi

  local versions

  case "${1}" in
    1.0*)
      versions=(100.14 162.01) #~
      ;;
    1.1*)
      versions=(169.01 169.01)
      ;;
    2.0*)
      versions=(177.70 177.70) #~
      ;;
    2.1*)
      versions=(180.22 180.22) #~
      ;;
    2.2*)
      versions=(185.18.14 185.18.14) #~
      ;;
    2.3*)
      versions=(190.53 190.53) #~
      ;;
    3.0*)
      versions=(195.36.15 195.36.15)
      ;;
    3.1*)
      versions=(256.40 256.40)
      ;;
    3.2*) # Package Version 3.2.16
      versions=(260.19.26 260.19.26)
      ;;
    4.0*) # Package Version 4.0.17
      versions=(270.41.19 270.41.19)
      ;;
    4.1*) # Package Version 4.1.28
      versions=(285.05.33 285.05.33)
      ;;
    4.2*) # Package Version 4.2.9
      versions=(295.41 295.41)
      ;;
    5.0*) # Package Version 5.0.35
      versions=(304 304)
      ;;
    5.5*) # CUDA Version 5.5.0, Package Version 5.5.22
      versions=(319 319)
      ;;
    6.0*) # CUDA Version 6.0.1, Package Version 6.0.37
      versions=(331 331)
      ;;
    6.5*) # CUDA Version 6.5.12, Package Version 6.5.14
      versions=(340 340)
      ;;
    7.0*) # CUDA Version 7.0.27, Package Version 7.0.28
      versions=(346.46 347.62)
      ;;
    7.5*) # CUDA Version 7.5.17, Package Version 7.5.18
      versions=(352.31 353.66)
      ;;
    8.0.44)
      versions=(367.48 369.30)
      ;;
    8.0*) # 8.0.61 GA2
      versions=(375.26 376.51)
      ;;
    9.0*) # 9.0.76 and RC2 9.0.103 and 9.0.176 and maybe RC 9.0.69 [2]
      versions=(384.81 385.54)
      # https://devtalk.nvidia.com/default/topic/1023719/cuda-setup-and-installation/-solved-cuda-9-0rc-and-nvidia-384-69-but-driver-version-is-insufficient-for-cuda-runtime-version/
      ;;
    9.1*) # 9.1.85
      versions=(390.46 391.29)
      # https://devtalk.nvidia.com/default/topic/1028802/cuda-setup-and-installation/problems-with-cuda-9-1-in-ubuntu-16-04/
      ;;
    9.2.88)
      versions=(396.26 397.44)
      ;;
    9.2*) # 9.2.148 Update 1
      versions=(396.37 398.26)
      ;;
    10.0*) #10.0.130
      versions=(410.48 411.31)
      ;;
    10.1*) #10.1.105/10.1.168 U1/10.1.243 U2
      versions=(418.39 418.96)
      ;;
    10.2*) #10.2.89
      versions=(440.33 441.22)
      ;;

    # Starting in CUDA 11, the "CUDA Version" and the component versions are no
    # longer the same, so that each component no longer needs to be updated when
    # nothing changes. However, this now means it's more complicated to match
    # the version number coming out of nvcc with the version of CUDA.
    # In General these case targets are "CUDA Version|nvcc version)"

    11.0.1|11.0.189) # there were multiple RCs, don't have all the info
      versions=(450.36.06 451.22)
      ;;
    11.0.2|11.0.194|11.0.207) # GA, docs inconsistently say 11.0.207
      versions=(450.51.05 451.48)
      ;;
    # 11.0.3|11.0.221|11.0.228) # Update 1, docs say 11.0.228
    11.0*)
      versions=(450.51.06 451.82)
      ;;

    11.1.0|11.1.74) # GA
      versions=(455.23 456.38)
      ;;
    # 11.1.1|11.1.105) Update 1
    11.1.*|11.1)
      versions=(455.32 456.81)
      ;;

    11.2.0|11.2.67)
      versions=(460.27.03 460.82)
      ;;
    11.2.1|11.2.142) # Update 1
      versions=(460.32.03 461.09)
      ;;
    # 11.2.2|11.2.152) # Update 2
    11.2*)
      versions=(460.32.03 461.33)
      ;;

    # 11.3.0|11.3.58) # GA
    # 11.3.1|11.3.109) # Update 1
    11.3*)
      versions=(465.19.0 465.89)
      ;;

    11.4.0|11.4.48)
      versions=(470.42.01 471.11)
      ;;
    11.4.1|11.4.100|11.4.2|11.4.120)
      versions=(470.57.02 471.41)
      ;;
    # 11.4.3|11.4.152)
    # 11.4.4|11.4.152)
    11.4*)
      versions=(470.82.01 472.50)
      ;;

    11.5.0|11.5.50)
      versions=(495.29.05 496.04)
      ;;
    # 11.5.1|11.5.119)
    # 11.5.2|11.5.119)
    11.5*)
      versions=(495.29.05 496.13)
      ;;

    11.6.0|11.6.55)
      versions=(510.39.01 511.23)
      ;;
    # 11.6.1|11.6.112)
    # 11.6.2|11.6.124)
    11.6*)
      versions=(510.47.03 511.65)
      ;;

    11.7.0|11.7.64)
      versions=(515.43.04 516.01)
      ;;
    # 11.7.1|11.7.99)
    11.7*)
      versions=(515.48.07 516.31)
      ;;

    # 11.8.0|11.8.89)
    11.8*)
      versions=(520.61.05 522.06)
      ;;

    12.0.0|12.0.76)
      versions=(525.60.13 527.41)
      ;;
    # 12.0.1|12.0.140)
    12.0*)
      versions=(525.85.12 528.33)
      ;;
    12.1.0|12.1.66)
      versions=(530.30.02 531.14)
      ;;
    # 12.1.1|12.1.105)
    12.1*)
      versions=(530.30.02 531.14)
      ;;
    12.2.0|12.2.91)
      versions=(535.54.03 536.25)
      ;;
    12.2.1|12.2.128)
      versions=(535.86.09 536.67)
      ;;
    # 12.2.2|12.2.140)
    12.2*)
      versions=(535.104.05 537.13)
      ;;
    12.3.0|12.3.52)
      versions=(545.23.06 545.84)
      ;;
    # 12.3.1|12.3.101)
    12.3*)
      versions=(545.23.08 546.12)
      ;;
    12.4.0|12.4.99)
      versions=(550.54.14 551.61)
      ;;
    # 12.4.1|12.4.127)
    12.4*)
      versions=(550.54.15 551.78)
      ;;
    12.5.0|12.5.40)
      versions=(555.42.02 555.85)
      ;;
    12.5.1|12.5.82)
      versions=(555.42.06 555.85)
      ;;
    12.6.0|12.6.20)
      versions=(560.28.03 560.76)
      ;;
    # 12.6.1|12.6.68)
    12.*)
      versions=(560.35.03 560.94)
      ;;

    # E.g. https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#id4
    # 12.7*|{nvcc)
    #   versions=(linux windows)
    #   ;;

    # These seem to be updated regularly
    # - https://stackoverflow.com/a/30820690/4166604
    # - https://gist.github.com/ax3l/9489132
    # - https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html
    # - https://docs.nvidia.com/deploy/cuda-compatibility/index.html also shows the minimal compatible versions

    *)
      echo "CUDA Version ${CUDA_VERSION} is not programmed in here yet..." >&2
      versions=(0 0)
      ;;
  esac

  if [ "${OS-}" == "Windows_NT" ]; then
    CUDA_MINIMAL_DRIVER_VERSION="${versions[1]}"
  else
    CUDA_MINIMAL_DRIVER_VERSION="${versions[0]}"
  fi

  versions=()

  # This is for CUDA_COMPATIBLE_DRIVER_VERSION
  case "${1}" in
    11.0*)
      versions=(450.36.06 451.22) # Actual min may be lower
      ;;
    11.*) # 11.1-11.8
      versions=(450.80.02 452.39)
      ;;
    12.[0-3].*)
      versions=(525.60.13 527.41)
      ;;
    12.*) # 12.4-12.6 attempted to retcon and hide 12.0-12.3
      versions=(525.60.13 528.33)
      ;;
    # E.g. https://docs.nvidia.com/cuda/archive/12.0.0/cuda-toolkit-release-notes/index.html#id3
    # 13.*)
    #   versions=(linux windows)
    #   ;;
  esac

  if [ "${#versions[@]}" = "2" ]; then
    if [ "${OS-}" == "Windows_NT" ]; then
      CUDA_COMPATIBLE_DRIVER_VERSION="${versions[1]}"
    else
      CUDA_COMPATIBLE_DRIVER_VERSION="${versions[0]}"
    fi
  fi
}

#**
# .. function:: suggested_architectures
#
# :Output: * :var:`cuda_info.bsh CUDA_SUGGESTED_ARCHES`
#          * :var:`cuda_info.bsh CUDA_SUGGESTED_CODES`
#          * :var:`cuda_info.bsh CUDA_SUGGESTED_PTX`
#          * :var:`cuda_info.bsh CUDA_FORWARD_PTX`
#
# Calculate suggested architectures
#**
function suggested_architectures()
{
  local x arch y
  local IFS=$'\n'
  local cuda_arches_sorted=($(sort -u <<< ${CUDA_ARCHES[*]+"${CUDA_ARCHES[*]}"}))
  local cuda_codes_sorted=($(sort -u <<< ${CUDA_CODES[*]+"${CUDA_CODES[*]}"}))
  CUDA_SUGGESTED_ARCHES=()
  CUDA_SUGGESTED_CODES=()
  CUDA_SUGGESTED_PTX=()

  # Loop through all the card aches
  for arch in ${CUDA_CARD_ARCHES[@]+"${CUDA_CARD_ARCHES[@]}"}; do
    for x in ${cuda_arches_sorted[@]+"${!cuda_arches_sorted[@]}"}; do
      # If it's an exact match, add it!
      if [ "${cuda_arches_sorted[x]}" == "${arch}" ]; then
        CUDA_SUGGESTED_ARCHES+=("${arch}")
        continue 2
      # If it's greater than, that means there is no exact match.
      # The last compute checked was the right answer. Use that.
      elif [ "${cuda_arches_sorted[x]}" -gt "${arch}" ]; then
        # Verify the compute family matches, else you probably have a card that
        # is too old, like a Fermi???
        if [ "${x}" -gt "0" ]; then
          CUDA_SUGGESTED_ARCHES+=("${cuda_arches_sorted[x-1]}")
        else
          : #: CUDA_SUGGESTED_ARCHES+=("TOOOLD")
        fi
        continue 2
      fi
    done

    # If you get here, either you have a newer arch of an already supported arch,
    # or you have a newer card than this version of cuda supports. Let's check
    # the first case first.
    if [ "${cuda_arches_sorted[x]:0:1}" == "${arch:0:1}" ]; then
      CUDA_SUGGESTED_ARCHES+=("${cuda_arches_sorted[x]}")
    else
      # If you get here, that means you have have a newer card than this version
      # of cuda supports. So the best answer is to PTX compile for it.
      CUDA_SUGGESTED_PTX+=("${cuda_arches_sorted[x]}")
    fi
  done

  for arch in ${CUDA_CARD_ARCHES[@]+"${CUDA_CARD_ARCHES[@]}"}; do
    for x in ${cuda_codes_sorted[@]+"${!cuda_codes_sorted[@]}"}; do
      # If it's an exact match, add it!
      if [ "${cuda_codes_sorted[x]}" == "${arch}" ]; then
        CUDA_SUGGESTED_CODES+=("${arch}")
        continue 2
      # If it's greater than, that means there is no exact match.
      # The last compute checked was the right answer. Use that.
      elif [ "${cuda_codes_sorted[x]}" -gt "${arch}" ]; then
        # Verify the compute family matches, else you probably have a card that
        # is too old, like a Fermi???
        if [ "${x}" == "0" ]; then
          # Too old
          : #CUDA_SUGGESTED_CODES+=("TOOOLD")
        else # if [ "${cuda_codes_sorted[$x-1]:0:1}" == "${arch:0:1}" ]; This should always be true
          CUDA_SUGGESTED_CODES+=("${cuda_codes_sorted[x-1]}")
        fi
        continue 2
      fi
    done

    # If you get here, either you have a newer arch of an already supported arch,
    # or you have a newer card than this version of cuda supports. Let's check
    # the first case first.
    if [ "${cuda_codes_sorted[x]:0:1}" == "${arch:0:1}" ]; then
      CUDA_SUGGESTED_CODES+=("${cuda_codes_sorted[x]}")
    # no else; PTX is for ARCH, not CODES
    fi
  done

  # This should not use the sorted list, as the last element it the highest OFFICIAL architecture
  CUDA_FORWARD_PTX="${CUDA_ARCHES[@]+"${CUDA_ARCHES[${#cuda_arches_sorted[@]}-1]}"}"

  CUDA_SUGGESTED_ARCHES=($(sort -u <<< ${CUDA_SUGGESTED_ARCHES[*]+"${CUDA_SUGGESTED_ARCHES[*]}"}))
  CUDA_SUGGESTED_CODES=($(sort -u <<< ${CUDA_SUGGESTED_CODES[*]+"${CUDA_SUGGESTED_CODES[*]}"}))
  CUDA_SUGGESTED_PTX=($(sort -u <<< ${CUDA_SUGGESTED_PTX[*]+"${CUDA_SUGGESTED_PTX[*]}"}))
}

#**
# .. function:: cmake_cuda_flags
#
# :Parameters: * :var:`cuda_info.bsh CUDA_SUGGESTED_ARCHES`
#              * :var:`cuda_info.bsh CUDA_SUGGESTED_CODES`
#              * [:var:`cuda_info.bsh CUDA_SUGGESTED_PTX`]
# :Output: *stdout* - echoes out the value of the target_CUDA_architectures
#
# Generate CUDA flags for CMake
#
# Modern CMake installs include a FindCUDA.cmake script which calls the select_compute_arch.cmake script (https://goo.gl/uZvAjR). It uses a limited version of the tables that :file:`cuda_info.bsh` uses and is prone to being out of date.
#
# This function will calculate the suggested value of target_CUDA_architectures for CMake's:
#
#   FindCUDA.cmake:select_compute_arch.cmake:CUDA_SELECT_NVCC_ARCH_FLAGS
#
# You will need to find where this is used and set the variable accordingly.
#
# .. rubric:: Example
#
# For example, PyTorch's CMake contains:
#
# .. code-block:: bash
#
#   CUDA_SELECT_NVCC_ARCH_FLAGS(NVCC_FLAGS_EXTRA $ENV{TORCH_CUDA_ARCH_LIST})
#
# Setting the environment variable ``TORCH_CUDA_ARCH_LIST`` to the output of :func:`cmake_cuda_flags` will result in using the desired CUDA architecture and code versions.
#
# To add the :var:`cuda_info.bsh CUDA_FORWARD_PTX`, run:
#
# .. code-block:: bash
#
#   CUDA_SUGGESTED_PTX+=(${CUDA_FORWARD_PTX})
#
# before calling :func:`cmake_cuda_flags`
#**
function cmake_cuda_flags()
{
  local cmake=()
  local x
  local y
  local ptx
  local OLD_IFS="${IFS}"

  for i in "${!CUDA_SUGGESTED_ARCHES[@]}"; do
    if [ "${CUDA_SUGGESTED_ARCHES[i]}" == "${CUDA_SUGGESTED_CODES[i]}" ]; then
      x="${CUDA_SUGGESTED_ARCHES[i]}"
      x="${x:0:${#x}-1}.${x:${#x}-1:1}"
    else
      x="${CUDA_SUGGESTED_CODES[i]}"
      y="${CUDA_SUGGESTED_ARCHES[i]}"
      x="${x:0:${#x}-1}.${x:${#x}-1:1}"
      y="${y:0:${#y}-1}.${y:${#y}-1:1}"
      x="${x}(${y})"
    fi
    cmake+=("${x}")
  done

  ptx=(${CUDA_SUGGESTED_PTX[@]+"${CUDA_SUGGESTED_PTX[@]}"})
  IFS=$'\n'
  ptx=($(echo "${ptx[*]+"${ptx[*]}"}" | sort -u))
  IFS="${OLD_IFS}"
  for i in "${!ptx[@]}"; do
    x="${CUDA_SUGGESTED_PTX[i]}"
    x="${x:0:${#x}-1}.${x:${#x}-1:1}"
    cmake+=("${x}+PTX")
  done

  echo -n ${cmake[*]+"${cmake[*]}"}
}

#**
# .. function:: cmake_cuda_architectures
#
# :Parameters: * :var:`cuda_info.bsh CUDA_SUGGESTED_ARCHES`
#              * [:var:`cuda_info.bsh CUDA_SUGGESTED_PTX`]
# :Output: *stdout* - semi-colon delimited string of CUDA capabilities for cmake
#
# Generate CUDA capabilities suitable for the ``CMAKE_CUDA_ARCHITECTURES`` environment variable.
# See cmake docs for more information https://cmake.org/cmake/help/latest/variable/CMAKE_CUDA_ARCHITECTURES.html
#
#**
function cmake_cuda_architectures()
{
  # check CUDA version
  if meet_requirements ${CUDA_VERSION-} '<=9.0'; then
    echo "CUDA version ${CUDA_VERSION-} must be greater than 9.0" >&2
    local JUST_IGNORE_EXIT_CODES=1
    return 1
  fi

  # cmake capabilties
  # https://cmake.org/cmake/help/latest/prop_tgt/CUDA_ARCHITECTURES.html
  local result=()
  local x=""

  for x in ${CUDA_SUGGESTED_ARCHES[@]+"${CUDA_SUGGESTED_ARCHES[@]}"}; do
    result+=( "${x}-real" )
  done

  for x in ${CUDA_SUGGESTED_PTX[@]+"${CUDA_SUGGESTED_PTX[@]}"}; do
    result+=( "${x}-virtual" )
  done

  # output
  local IFS=';'
  echo -n ${result[*]+"${result[*]}"}
}

#**
# .. function:: torch_cuda_arch_list
#
# :Parameters: * :var:`cuda_info.bsh CUDA_SUGGESTED_ARCHES`
#              * [:var:`cuda_info.bsh CUDA_SUGGESTED_PTX`]
# :Output: *stdout* - comma delimited string of CUDA architectures for pytorch
#
# Generate CUDA capabilities suitable for the ``TORCH_CUDA_ARCH_LIST`` environment variable.
# See pytorch docs for more information https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.CUDAExtension
#
#**
function torch_cuda_arch_list()
{
  # check CUDA version
  if meet_requirements ${CUDA_VERSION-} '<=9.0'; then
    echo "CUDA version ${CUDA_VERSION-} must be greater than 9.0" >&2
    local JUST_IGNORE_EXIT_CODES=1
    return 1
  fi

  # TORCH_CUDA_ARCH_LIST
  # https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.CUDAExtension
  local result=()
  local x=""

  for x in ${CUDA_SUGGESTED_ARCHES[@]+"${CUDA_SUGGESTED_ARCHES[@]}"}; do
    if ! isin "${x}" ${CUDA_SUGGESTED_PTX[@]+"${CUDA_SUGGESTED_PTX[@]}"}; then
      result+=( "${x::1}.${x:1}" )
    fi
  done

  for x in ${CUDA_SUGGESTED_PTX[@]+"${CUDA_SUGGESTED_PTX[@]}"}; do
    result+=( "${x::1}.${x:1}+PTX" )
  done

  # output
  echo -n ${result[*]+"${result[*]}"}
}

#**
# .. function:: torch_cuda_arch_list_from_cmake
#
# :Arguments: * ``$1`` - delimited string of cuda architectures
# :Output: *stdout* - string containing ``TORCH_CUDA_ARCH_LIST`` statement
#
# Create ``TORCH_CUDA_ARCH_LIST`` from ``CMAKE_CUDA_ARCHITECTURES`` statement.
#
# .. rubric:: Example
#
# For example, an input of ``"80-real 86-virtual"`` would produce the output ``"8.0 8.6+PTX"``
#**
function torch_cuda_arch_list_from_cmake()
{
  # input as space delimited string
  local input="${1//[,;]/ }"

  # generate TORCH_CUDA_ARCH_LIST statements
  local x=""
  local capability=""
  local result=()
  for x in ${input}; do

    # X.Y capability
    capability="${x//[!0-9]/}"
    capability="${capability::1}.${capability:1}"

    # append PTX
    if [[ ${x} = *+PTX ]]; then
      capability+="+PTX"
    elif [[ ${x} = *-virtual ]]; then
      capability+="+PTX"
    fi

    result+=( "${capability}" )
  done

  # output
  echo -n ${result[*]+"${result[*]}"}
}

#**
# .. function:: tcnn_cuda_architectures
#
# :Parameters: * :var:`cuda_info.bsh CUDA_SUGGESTED_ARCHES`
#              * [:var:`cuda_info.bsh CUDA_SUGGESTED_PTX`]
# :Output: *stdout* - comma delimited string of CUDA architectures for tiny-cuda-nn
#
# Generate CUDA architectures for tiny-cuda-nn, suitable for the ``TCNN_CUDA_ARCHITECTURES`` environment variable. Note tiny-cuda-nn will always build both binaries and PTX intermediate code for each specified architecture.
#
#**
function tcnn_cuda_architectures()
{
  # check CUDA version
  if meet_requirements ${CUDA_VERSION-} '<=9.0'; then
    echo "CUDA version ${CUDA_VERSION-} must be greater than 9.0" >&2
    local JUST_IGNORE_EXIT_CODES=1
    return 1
  fi

  # append suggested arches & ptx
  local result=(
    ${CUDA_SUGGESTED_ARCHES[@]+"${CUDA_SUGGESTED_ARCHES[@]}"}
    ${CUDA_SUGGESTED_PTX[@]+"${CUDA_SUGGESTED_PTX[@]}"}
  )

  # unique sorted values
  local IFS=$'\n'
  result=($(sort -u <<< ${result[*]+"${result[*]}"}))

  # output
  IFS=','
  echo -n ${result[*]+"${result[*]}"}
}

#**
# .. function:: tcnn_cuda_architectures_from_cmake
#
# :Arguments: * ``$1`` - delimited string of cuda architectures
# :Output: *stdout* - string containing ``TCNN_CUDA_ARCHITECTURES`` statement
#
# Create ``TCNN_CUDA_ARCHITECTURES`` from ``CMAKE_CUDA_ARCHITECTURES`` statement.
#
# .. rubric:: Example
#
# For example, an input of ``"80-real 86-virtual"`` would produce the output ``"80 86"``
#**
function tcnn_cuda_architectures_from_cmake()
{
  # input as space delimited string
  local input="${1//[,;]/ }"

  # generate TCNN_CUDA_ARCHITECTURES statements
  local x=""
  local result=()
  for x in ${input}; do
    result+=( "${x//[!0-9]/}" )
  done

  # unique sorted values
  local IFS=$'\n'
  result=($(sort -u <<< ${result[*]+"${result[*]}"}))

  # output
  IFS=','
  echo -n ${result[*]+"${result[*]}"}
}

#**
# .. function:: nvcc_gencodes
#
# :Arguments: * ``$1`` - delimited string of cuda architectures
# :Output: *stdout* - string containing nvcc gencode statements
#
# Create nvcc gencode statements from input array of cuda capabilities.
#
# .. rubric:: Example
#
# For example, an input of ``"75 86+PTX"`` would produce the output
#
# .. code-block:: bash
#
#   -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_86,code=compute_86
#
#**
function nvcc_gencodes()
{
  # input as space delimited string
  local input="${1//[,;]/ }"

  # remove any periods from cuda architectures (e.g., 7.5->75)
  input="${input//./}"

  # input as array
  local capabilities=( ${input} ) # noquotes

  # generate gencode statements
  local gencodes=()
  local capability=""
  local arch=""
  local code=""
  for capability in "${capabilities[@]}"; do
    if [[ ${capability} = *+PTX ]]; then
      capability="${capability%+PTX}"
      arch="compute_${capability}"
      code="compute_${capability}"
    elif [[ ${capability} = *-real ]]; then
      capability="${capability%-real}"
      arch="compute_${capability}"
      code="sm_${capability}"
    elif [[ ${capability} = *-virtual ]]; then
      capability="${capability%-virtual}"
      arch="compute_${capability}"
      code="compute_${capability}"
    else
      arch="compute_${capability}"
      code="sm_${capability}"
    fi
    gencodes+=( "-gencode=arch=${arch},code=${code}" )
  done

  # output
  echo -n ${gencodes[*]+"${gencodes[*]}"}
}

#**
# .. function:: discover_cuda_all
#
# Helper function to call all the discovery code in one call.
#
# There are two methods for CUDA device discovery (in order):
#
# 1. Using deviceQuery (available here https://goo.gl/ocBgPU)
#
#   * looks for ${DEVICE_QUERY-deviceQuery} on the PATH
#
# 2. Using the nvidia-docker-plugin to get and parse GPU information
#
#   * discovered using either ``NV_HOST`` or checking to see if nvidia-docker-plugin is running locally using pgrep or ps
#
# When running in a docker, deviceQuery is the preferred method. ``NV_HOST`` could be used, but that involves telling the docker the IP of the host, or using a shared network mode in order to use localhost (which is not recommended for production). Attempting to discover nvidia-docker-plugin will not work in a docker.
#
# :Output: * :var:`cuda_info.bsh CUDA_CARDS`
#          * :var:`cuda_info.bsh CUDA_CARD_ARCHES`
#          * :var:`cuda_info.bsh CUDA_CARD_FAMILIES`
#
# .. note::
#
#    If deviceQuery is not used, then an internal lookup table is used, but only supports Tesla, Fermi, Kepler, Maxwell, Pascal, Volta, Tesla, and Ampere. Additional family names need to be added as they are released.
#**
function discover_cuda_all()
{
  # Call all of the CUDA discovery functions
  if [ -z "${CUDA_VERSION+set}" ]; then
    discover_cuda_versions
  fi

  if [ -n "${CUDA_VERSION+set}" ]; then
    discover_cuda_info
    cuda_capabilities "${CUDA_VERSION}"
    suggested_architectures
  else
    echo "Could not determine a CUDA version, please set CUDA_VERSION and call again" >&2
  fi
}

#**
# .. function:: has_gpu_device
#
# Tells you if the computer has any NVIDIA GPUs. This is checking if they physically exist, not if they are currently working or have working drivers. This is usually used to determine if you need to install drivers, not if you have GPUs ready to use.
#
# :Return Value: * ``0`` - The machine has an NVIDIA GPU.
#                * ``1`` - No NVIDIA GPUs were found.
#
# .. seealso::
#   :func:`is_gpu_setup`
#**
function has_gpu_device()
{
  if [ -n "${WSL_INTEROP+set}" -o "${OS-}" = "Windows_NT" ]; then
    if [ "$(powershell.exe -NoProfile -InputFormat None '(Get-WMIObject win32_pnpEntity -Filter {PNPClass = "Display" and Manufacturer like "nvidia"} | Measure-Object).Count')" -gt 0 ]; then
      return 0
    fi
  else
    local dev vendor config
    # 1. Loop over /sys/bus/pci/devices/*
    for dev in /sys/bus/pci/devices/*; do
      # 2. Read vendor, == "0x10de"
      vendor=$(cat "${dev}/vendor")
      if [ "${vendor}" = "0x10de" ]; then
        # 3. Read the 10th and 11th byte of config == 00 03, say "gpu" if it is
        # Note: This sed expression does not work on BSD (mac) sed, but that won't be an issue here
        if [ "$(LC_ALL=C sed -nE $'/^.{10}\\x00\\x03/i gpu\nq' "${dev}/config")" == "gpu" ]; then
        # Awk version of same check: LC_ALL=C awk 'NR=2{ x=substr($0, 11, 2); if ( x == "\x00\x03") {exit 0} else {exit 1} }'
          return 0
        fi
      fi
    done
  fi
  return 1
}

#**
# .. function:: is_gpu_setup
#
# Checks to see if you have any GPUs installed and working.
#
# :Return Value: * ``0`` - There is at least one NVIDIA GPU available to use.
#                * ``1`` - No NVIDIA GPUs were available.
#
# - If there is a problem with the NVIDIA drivers resulting in the kernel module not loading, then this check will return false.
# - If you are in a docker and did not route the GPU in correctly, it will return false.
#
# .. seealso::
#   :func:`has_gpu_device`
#**
function is_gpu_setup()
{
  if [ "${OS-}" = "Windows_NT" -a -z "${WSL_INTEROP+set}" ]; then
    # Check on linux
    if wmic path win32_VideoController get name | grep -qi nvidia; then
      return 0
    fi
  # Check in WSL/WSL docker or linux comptuer
  elif [ -c "/dev/dxg" ] || compgen -G "/dev/nvidia[0-9]*" &> /dev/null ; then
    return 0
  fi
  return 1
}
